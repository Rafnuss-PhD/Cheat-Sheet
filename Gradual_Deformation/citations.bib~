@article{Yaramanci2005,
author = {Yaramanci, U and Kemna, A and Vereecken, H},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Yaramanci, Kemna, Vereecken - 2005 - Emerging technologies in hydrogeophysics.pdf:pdf},
journal = {Hydrogeophysics},
pages = {467--486},
title = {{Emerging technologies in hydrogeophysics}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_16},
year = {2005}
}
@article{Gomes-Hernandez2005,
author = {Gomes-Hernandez, J James},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Gomes-Hernandez - 2005 - Geostatistics.pdf:pdf},
journal = {Hydrogeophysics},
pages = {59--83},
title = {{Geostatistics}},
year = {2005}
}
@inproceedings{roggero1998gradual,
author = {Roggero, F and Hu, Lin Y.},
booktitle = {SPE annual technical conference},
pages = {221--236},
title = {{Gradual deformation of continuous geostatistical models for history matching}},
year = {1998}
}
@inproceedings{hu1998constraining,
author = {Hu, Lin Y. and Blanc, G},
booktitle = {6th European Conference on the Mathematics of Oil Recovery},
title = {{Constraining a reservoir facies model to dynamic data using a gradual deformation method}},
year = {1998}
}
@article{srivastava1994interactive,
author = {Srivastava, R M and Others},
journal = {SPE},
pages = {87--95},
title = {{The interactive visualization of spatial uncertainty}},
volume = {27965},
year = {1994}
}
@book{journel1978mining,
author = {Journel, Andre G and Huijbregts, Ch J},
publisher = {Academic press},
title = {{Mining geostatistics}},
year = {1978}
}
@article{Oliver1997,
abstract = {Generating one realization of a random permeability field that is consistent with observed pressure data and a known variogram model is not a difficult problem. If, however, one wants to investigate the uncertainty of reservior behavior, one must generate a large number of realizations and ensure that the distribution of realizations properly reflects the uncertainty in reservoir properties. The most widely used method for conditioning permeability fields to production data has been the method of simulated annealing, in which practitioners attempt to minimize the difference between the ’ ’true and simulated production data, and “true” and simulated variograms. Unfortunately, the meaning of the resulting realization is not clear and the method can be extremely slow. In this paper, we present an alternative approach to generating realizations that are conditional to pressure data, focusing on the distribution of realizations and on the efficiency of the method. Under certain conditions that can be verified easily, the Markov chain Monte Carlo method is known to produce states whose frequencies of appearance correspond to a given probability distribution, so we use this method to generate the realizations. To make the method more efficient, we perturb the states in such a way that the variogram is satisfied automatically and the pressure data are approximately matched at every step. These perturbations make use of sensitivity coefficients calculated from the reservoir simulator.},
author = {Oliver, Dean S. and Cunha, Luciane B. and Reynolds, Albert C.},
doi = {10.1007/BF02769620},
file = {:home/raphael/Documents/Paper-Mendeley/1997/Oliver, Cunha, Reynolds - 1997 - Markov chain Monte Carlo methods for conditioning a permeability field to pressure data.pdf:pdf},
issn = {0882-8121},
journal = {Mathematical Geology},
keywords = {conditional simulation,markov chain,monte carlo,pressure data,sampling,sensitivity,well test},
number = {1},
pages = {61--91},
title = {{Markov chain Monte Carlo methods for conditioning a permeability field to pressure data}},
volume = {29},
year = {1997}
}
@incollection{deutsch1993conditioning,
author = {Deutsch, Clayton V},
booktitle = {Geostatistics Tr\{\'{o}\}ia’92},
pages = {505--518},
publisher = {Springer},
title = {{Conditioning reservoir models to well test information}},
year = {1993}
}
@article{Goffe1994,
abstract = {Many statistical methods rely on numerical optimization to estimate a model's parameters. Unfortunately, conventional algorithms sometimes fail. Even when they do converge, there is no assurance that they have found the global, rather than a local, optimum. We test a new optimization algorithm, simulated annealing, on four econometric problems and compare it to three common conventional algorithms. Not only can simulated annealing find the global optimum, it is also less likely to fail on difficult functions because it is a very robust algorithm. The promise of simulated annealing is demonstrated on the four econometric problems.},
author = {Goffe, William L. and Ferrier, Gary D. and Rogers, John},
doi = {10.1016/0304-4076(94)90038-8},
file = {:home/raphael/Documents/Paper-Mendeley/1994/Goffe, Ferrier, Rogers - 1994 - Global optimization of statistical functions with simulated annealing.pdf:pdf},
isbn = {0304-4076},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {simulated,simulated annealing},
mendeley-tags = {simulated annealing},
pages = {65--99},
title = {{Global optimization of statistical functions with simulated annealing}},
volume = {60},
year = {1994}
}
@misc{Doyen1995,
abstract = {A discretized lithologic model of the subsurface is de ?ned by a regular array of pixels. Each pixel corre sponds to one of a ?nite number of possible lithoclasses such as sand, shale or dolomite. The lithoclasses are unknown except at a small number of sparsely distrib- - uted control pixels associated with borehole locations. Associated with each pixel there is a multivariate re cord of seismic attributes that may be statistically corre latable with the local lithology. A Monte Carlo method is used to simulate the lithoclass spatial distribution by combining the lithologic data at control pixels with the seismic-attribute data records. Using Indicator Kriging, a prior probability distribution of the lithoclasses is calculated for each pixel from the lithology values at neighboring pixels. The likelihood of each lithoclass is also calculated in each pixel from the corresponding conditional probability distribution of seismic attributes. A posterior lithoclass probability distribution is ob tained at each pixel by multiplying the prior distribution and the likelihood function. The posterior distributions are sampled pixel-by-pixel to generate equally probable models of the subsurface lithology.},
author = {Doyen, Philippe M and Psaila, David E},
file = {:home/raphael/Documents/Paper-Mendeley/1995/Doyen, Psaila - 1995 - Bayesian sequential indicator simulation of lithology from seismic data.pdf:pdf},
title = {{Bayesian sequential indicator simulation of lithology from seismic data}},
year = {1995}
}
@article{Iii1996,
abstract = {A multivariate stochastic simulation application that involves the mapping of a primary variable from a combi nation for sparse primary data and more densely sampled secondary data. The method is applicable when the relation ship between the simulated primary variable and one or more secondary variables is non-linear. The method employs a Bayesian updating rule to build a local posterior distribution for the primary variable at each simulated location. The posterior distribution is the product of a Gaussian kernel function obtained by simple kriging of the primary variable and a secondary probability function obtained directly from a scatter diagram between primary and secondary variables.},
annote = {Overall graph of Bayasian sequential simulation very good.},
author = {Doyen, Philippe M and {Den Boer}, Lennert D},
file = {:home/raphael/Documents/Paper-Mendeley/1996/Doyen, Den Boer - 1996 - Bayesian sequential Gaussian simulation of lithology with non-linear data.pdf:pdf},
keywords = {Bayes' theory,sequential simulation},
mendeley-tags = {Bayes' theory,sequential simulation},
title = {{Bayesian sequential Gaussian simulation of lithology with non-linear data}},
year = {1996}
}
@article{Renard1997,
abstract = {The purpose of this article is to review the various methods used to calculate the equivalent permeability of a heterogeneous porous medium. It shows how equivalence is defined by using a criterion of flow or of the energy dissipated by viscous forces and explains the two different concepts of effective permeability and block permeability. The intention of this review is to enable the reader to use the various published techniques and to indicate in what circumstances they can be most suitably applied. © 1997 Elsevier Science Ltd. All rights reserved},
author = {Renard, Philippe and de Marsily, G.},
doi = {10.1016/S0309-1708(96)00050-4},
file = {:home/raphael/Documents/Paper-Mendeley/1997/Renard, de Marsily - 1997 - Calculating equivalent permeability a review.pdf:pdf},
isbn = {0309-1708},
issn = {03091708},
journal = {Advances in Water Resources},
pages = {253--278},
title = {{Calculating equivalent permeability: a review}},
volume = {20},
year = {1997}
}
@article{LeRavalec2000,
abstract = {A fast Fourier transform (FFT) moving average (FFT-MA) method for generating Gaussian stochastic processes is derived. Using discrete Fourier transforms makes the calculations easy and fast so that large random fields can be produced. On the other hand, the basic moving average frame allows its to uncouple the random numbers from the structural parameters (mean, variance, correlation length,... ), but also to draw the randomness components in spatial domain. Such features impart great flexibility to the FFT-MA generator: For instance, changing only the random numbers gives distinct realizations all having the same covariance function. Similarly several, realizations can be built from the same random number set, but from a different structural parameters. Integrating the FFT-MA generator into an optimization procedure provides a tool theoretically capable to determine the random numbers identifying the Gaussian field as rr cll ns the structural parameters from dynamic data. Moreover, all or only some of the random numbers can be perturbed so that realizations produced using the FFT-MA generator carl be locally updated through an optimization process.},
author = {{Le Ravalec-Dupin}, Micka\"{e}le and Noetinger, Beno\^{\i}t and Hu, Lin Y.},
doi = {10.1023/A:1007542406333},
file = {:home/raphael/Documents/Paper-Mendeley/2000/Le Ravalec-Dupin, Noetinger, Hu - 2000 - The FFT moving average (FFT-MA) generator An efficient numerical method for generating and cond.pdf:pdf},
isbn = {0882-8121},
issn = {08828121},
journal = {Mathematical Geology},
keywords = {FFT,Local perturbation,Nonlinear conditioning,Optimization,Simulation},
number = {6},
pages = {701--723},
title = {{The FFT moving average (FFT-MA) generator: An efficient numerical method for generating and conditioning Gaussian simulations}},
volume = {32},
year = {2000}
}
@article{Strebelle2002,
abstract = {In many earth sciences applications, the geological objects or structures to be reproduced are curvilinear, e.g., sand channels in a elastic reservoir. Their modeling requires multiple-point statistics involving jointly three or more points at a time, much beyond the traditional two-point variogram statistics, Actual data from the field being modeled, particulary, if it is subsurface, are rarely enough to allow inference of such multiple-point statistics. The approach proposed in this paper consists of borrowing the required multiple-point statistics from training images depicting the expected patterns of geological heterogeneities. Several training images can be used, reflecting different scales of variability and styles of heterogeneities. The multiple-point statistics inferred from these training image(s) are exported to the geostatistical numerical model where they are anchored to the actual data, both hard and soft, in a sequential simulation mode. The algorithm and code developed are. tested for the simulation of a fluvial hydrocarbon reservoir with meandering channels. The methodology proposed appears to be simple (multiple-point statistics are scanned directly from training images), general (any type of random geometry, can be considered), and fast enough to handle large 3D simulation grids.},
author = {Strebelle, Sebastien},
doi = {10.1023/A:1014009426274},
file = {:home/raphael/Documents/Paper-Mendeley/2002/Strebelle - 2002 - Conditional simulation of complex geological structures using multiple-point statistics.pdf:pdf},
isbn = {0882-8121},
issn = {08828121},
journal = {Mathematical Geology},
keywords = {Geostatistics,Random geometry,Stochastic simulation,Training image},
number = {1},
pages = {1--21},
title = {{Conditional simulation of complex geological structures using multiple-point statistics}},
volume = {34},
year = {2002}
}
@article{LeRavalec-Dupin2002,
abstract = {.},
author = {{Le Ravalec-Dupin}, Micka\"{e}le and Noetinger, Beno\^{\i}t},
doi = {10.1023/A:1014408117518},
file = {:home/raphael/Documents/Paper-Mendeley/2002/Le Ravalec-Dupin, Noetinger - 2002 - Optimization with the gradual deformation method.pdf:pdf},
issn = {08828121},
journal = {Mathematical Geology},
keywords = {Convergence,Gradual deformation,Optimization,gradient-based optimization,gradual deformation,history matching,optimization},
mendeley-tags = {gradient-based optimization,gradual deformation,history matching,optimization},
number = {2},
pages = {125--142},
title = {{Optimization with the gradual deformation method}},
volume = {34},
year = {2002}
}
@article{Journel2002,
abstract = {Consider the assessment of any unknown event A through its conditional probability P(A | B,C) given two data events B, C of different sources. Each event could involve many locations jointly, but the two data events are assumed such that the probabilities P(A | B) and P(A | C) can be evaluated. The challenge is to recombine these two partially conditioned probabilities into a model for P(A | B,C) without having to assume independence of the two data events B and C. The probability P(A | B,C) is then used for estimation or simulation of the event A. In presence of actual data dependence, the combination algorithm provided by the traditional conditional independence hypothesis is shown to be nonrobust leading to various inconsistencies. An alternative based on a permanence of updating ratios is proposed, which guarantees all limit conditions even in presence of complex data interdependence. The resulting recombination formula is extended to any number n of data events and a paradigm is offered to introduce formal data interdependence.},
author = {Journel, Andre G},
doi = {10.1023/A:1016047012594},
file = {:home/raphael/Documents/Paper-Mendeley/2002/Journel - 2002 - Combining knowledge from diverse sources An alternative to traditional data independence hypotheses.pdf:pdf},
isbn = {08828121},
issn = {08828121},
journal = {Mathematical Geology},
keywords = {Conditional independence,Conditional probability,Data integration,Permanence of ratios},
number = {5},
pages = {573--596},
title = {{Combining knowledge from diverse sources: An alternative to traditional data independence hypotheses}},
volume = {34},
year = {2002}
}
@article{Renard2007,
abstract = {Quantitative hydrogeology celebrated its 150th anniversary in 2006. Geostatistics is younger but has had a very large impact in hydrogeology. Today, geostatistics is used routinely to interpolate deterministically most of the parameters that are required to analyze a problem or make a quantitative analysis. In a small number of cases, geostatistics is combined with deterministic approaches to forecast uncertainty. At a more academic level, geostatistics is used extensively to study physical processes in heterogeneous aquifers. Yet, there is an important gap between the academic use and the routine applications of geostatistics. The reasons for this gap are diverse. These include aspects related to the hydrogeology consulting market, technical reasons such as the lack of widely available software, but also a number of misconceptions. A change in this situation requires acting at different levels. First, regulators must be convinced of the benefit of using geostatistics. Second, the economic potential of the approach must be emphasized to customers. Third, the relevance of the theories needs to be increased. Last, but not least, software, data sets, and computing infrastructure such as grid computing need to be widely available.},
author = {Renard, Philippe},
doi = {10.1111/j.1745-6584.2007.00340.x},
file = {:home/raphael/Documents/Paper-Mendeley/2007/Renard - 2007 - Stochastic hydrogeology What professionals really need.pdf:pdf},
isbn = {1745-6584},
issn = {0017467X},
journal = {Ground Water},
number = {5},
pages = {531--541},
pmid = {17760580},
title = {{Stochastic hydrogeology: What professionals really need?}},
volume = {45},
year = {2007}
}
@article{Suzuki2008,
abstract = {Spatial inverse problems in the Earth Sciences are often ill-posed, requir- ing the specification of a prior model to constrain the nature of the inverse solutions. Otherwise, inverted model realizations lack geological realism. In spatial modeling, such prior model determines the spatial variability of the inverse solution, for ex- ample as constrained by a variogram, a Boolean model, or a training image-based model. In many cases, particularly in subsurface modeling, one lacks the amount of data to fully determine the nature of the spatial variability. For example, many differ- ent training images could be proposed for a given study area. Such alternative training images or scenarios relate to the different possible geological concepts each exhibit- ing a distinctive geological architecture. Many inverse methods rely on priors that represent a single subjectively chosen geological concept (a single variogram within a multi-Gaussian model or a single training image). This paper proposes a novel and practical parameterization of the prior model allowing several discrete choices of ge- ological architectures within the prior. This method does not attempt to parameterize the possibly complex architectures by a set of model parameters. Instead, a large set of prior model realizations is provided in advance, by means of Monte Carlo simu- lation, where the training image is randomized. The parameterization is achieved by defining ametric space which accommodates this large set of model realizations. This metric space is equipped with a “similarity distance” function or a distance function that measures the similarity of geometry between any two model realizations relevant to the problem at hand. Through examples, inverse solutions can be efficiently found in this metric space using a simple stochastic search method.},
author = {Suzuki, Satomi and Caers, Jef},
doi = {10.1007/s11004-008-9154-8},
file = {:home/raphael/Documents/Paper-Mendeley/2008/Suzuki, Caers - 2008 - A distance-based prior model parameterization for constraining solutions of spatial inverse problems.pdf:pdf},
issn = {18748961},
journal = {Mathematical Geosciences},
keywords = {Distance function,Geostatistics,Inverse problems,Stochastic search},
pages = {445--469},
title = {{A distance-based prior model parameterization for constraining solutions of spatial inverse problems}},
volume = {40},
year = {2008}
}
@article{Scheidt2009,
abstract = {Assessing uncertainty of a spatial phenomenon requires the analysis$\backslash$nof a large number of parameters which must be processed by a transfer$\backslash$nfunction. To capture the possibly of a wide range of uncertainty$\backslash$nin the transfer function response, a large set of geostatistical$\backslash$nmodel realizations needs to be processed. Stochastic spatial simulation$\backslash$ncan rapidly provide multiple, equally probable realizations. However,$\backslash$nsince the transfer function is often computationally demanding, only$\backslash$na small number of models can be evaluated in practice, and are usually$\backslash$nselected through a ranking procedure. Traditional ranking techniques$\backslash$nfor selection of probabilistic ranges of response (P10, P50 and P90)$\backslash$nare highly dependent on the static property used. In this paper,$\backslash$nwe propose to parameterize the spatial uncertainty represented by$\backslash$na large set of geostatistical realizations through a distance function$\backslash$nmeasuring dissimilarity between any two geostatistical realizations.$\backslash$nThe distance function allows a mapping of the space of uncertainty.$\backslash$nThe distance can be tailored to the particular problem. The multi-dimensional$\backslash$nspace of uncertainty can be modeled using kernel techniques, such$\backslash$nas kernel principal component analysis (KPCA) or kernel clustering.$\backslash$nThese tools allow for the selection of a subset of representative$\backslash$nrealizations containing similar properties to the larger set. Without$\backslash$nlosing accuracy, decisions and strategies can then be performed applying$\backslash$na transfer function on the subset without the need to exhaustively$\backslash$nevaluate each realization. This method is applied to a synthetic$\backslash$noil reservoir, where spatial uncertainty of channel facies is modeled$\backslash$nthrough multiple realizations generated using a multi-point geostatistical$\backslash$nalgorithm and several training images.},
author = {Scheidt, C\'{e}line and Caers, Jef},
doi = {10.1007/s11004-008-9186-0},
file = {:home/raphael/Documents/Paper-Mendeley/2009/Scheidt, Caers - 2009 - Representing spatial uncertainty using distances and kernels.pdf:pdf},
isbn = {1100400891860},
issn = {18748961},
journal = {Mathematical Geosciences},
keywords = {Distance,Geostatistics,Kernel methods,Ranking,Uncertainty quantification},
pages = {397--419},
title = {{Representing spatial uncertainty using distances and kernels}},
volume = {41},
year = {2009}
}
@article{Mariethoz2010a,
abstract = {Measurements are often unable to uniquely characterize the subsurface at a desired modeling resolution. In particular, inverse problems involving the characterization of hydraulic properties are typically ill-posed since they generally present more unknowns than data. In a Bayesian context, solutions to such problems consist of a posterior ensemble of models that fit the data (up to a certain precision specified by a likelihood function) and that are a subset of a prior distribution. Two possible approaches for this problem are Markov chain Monte Carlo (McMC) techniques and optimization (calibration) methods. Both frameworks rely on a perturbation mechanism to steer the search for solutions. When the model parameters are spatially dependent variable fields obtained using geostatistical realizations, such as hydraulic conductivity or porosity, it is not trivial to incur perturbations that respect the prior spatial model. To overcome this problem, we propose a general transition kernel (iterative spatial resampling, ISR) that preserves any spatial model produced by conditional simulation. We also present a stochastic stopping criterion for the optimizations inspired from importance sampling. In the studied cases, this yields posterior distributions reasonably close to the ones obtained by a rejection sampler, but with a greatly reduced number of forward model runs. The technique is general in the sense that it can be used with any conditional geostatistical simulation method, whether it generates continuous or discrete variables. Therefore it allows sampling of different priors and conditioning to a variety of data types. Several examples are provided based on either multi-Gaussian or multiple-point statistics. © 2010 by the American Geophysical Union.},
author = {Mariethoz, Gr\'{e}goire and Renard, Philippe and Caers, Jef},
doi = {10.1029/2010WR009274},
file = {:home/raphael/Documents/Paper-Mendeley/2010/Mariethoz, Renard, Caers - 2010 - Bayesian inverse problem and optimization with iterative spatial resampling.pdf:pdf},
issn = {00431397},
journal = {Water Resources Research},
number = {July},
pages = {1--17},
title = {{Bayesian inverse problem and optimization with iterative spatial resampling}},
volume = {46},
year = {2010}
}
@article{Everett2005,
author = {Everett, ME and Meju, MA},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Everett, Meju - 2005 - Near-surface controlled-source electromagnetic induction.pdf:pdf},
journal = {Hydrogeophysics},
pages = {157--183},
title = {{Near-surface controlled-source electromagnetic induction}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_6},
year = {2005}
}
@incollection{Binley2005,
author = {Binley, A and Kemna, A},
booktitle = {Hydrogeophysics},
editor = {{Rubin, Yoram and Hubbard}, Susan Sharpless},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Binley, Kemna - 2005 - DC resistivity and induced polarization methods.pdf:pdf},
pages = {129--156},
publisher = {Springer},
title = {{DC resistivity and induced polarization methods}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_5},
year = {2005}
}
@article{Rubin2005,
author = {Rubin, Y and Hubbard, SS},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Rubin, Hubbard - 2005 - Stochastic forward and inverse modeling The “hydrogeophysical” challenge.pdf:pdf},
journal = {Hydrogeophysics},
pages = {487--511},
title = {{Stochastic forward and inverse modeling: The “hydrogeophysical” challenge}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_17},
year = {2005}
}
@article{Mariethoz2010,
abstract = {Multiple-point geostatistics is a general statistical framework to model spatial fields displaying a wide range of complex structures. In particular, it allows controlling connectivity patterns that have a critical importance for groundwater flow and transport problems. This approach involves considering data events (spatial arrangements of values) derived from a training image (TI). All data events found in the TI are usually stored in a database, which is used to retrieve conditional probabilities for the simulation. Instead, we propose to sample directly the training image for a given data event, making the database unnecessary. Our method is statistically equivalent to previous implementations, but in addition it allows extending the application of multiple-point geostatistics to continuous variables and to multivariate problems. The method can be used for the simulation of geological heterogeneity, accounting or not for indirect observations such as geophysics. We show its applicability in the presence of complex features, nonlinear relationships between variables, and with various cases of nonstationarity. Computationally, it is fast, easy to parallelize, parsimonious in memory needs, and straightforward to implement.},
author = {Mariethoz, Gr\'{e}goire and Renard, Philippe and Straubhaar, Julien},
doi = {10.1029/2008WR007621},
file = {:home/raphael/Documents/Paper-Mendeley/2010/Mariethoz, Renard, Straubhaar - 2010 - The direct sampling method to perform multiple-point geostatistical simulations.pdf:pdf},
issn = {00431397},
journal = {Water Resources Research},
keywords = {DS,cosimulation,doi:10.1029/2008WR007621,geostatistics,http://dx.doi.org/10.1029/2008WR007621,hydrogeology,multivariate,syn-processing},
number = {July},
pages = {1--14},
title = {{The direct sampling method to perform multiple-point geostatistical simulations}},
volume = {46},
year = {2010}
}
@article{Goff1999,
author = {Goff, John a. and Jennings, James W.},
doi = {10.1023/A:1007524209849},
file = {:home/raphael/Documents/Paper-Mendeley/1999/Goff, Jennings - 1999 - Improvement of Fourier-based unconditional and conditional simulations for band limited fractal (von Karman) sta.pdf:pdf},
issn = {08828121},
journal = {Mathematical Geology},
keywords = {Band-limited fractal,Conditional simulation,Fast kriging,Fourier methods,Simulation,Variogram},
number = {6},
pages = {627--649},
title = {{Improvement of Fourier-based unconditional and conditional simulations for band limited fractal (von Karman) statistical models}},
volume = {31},
year = {1999}
}
@article{Hu2000,
abstract = {This paper describes a newmethod for gradually deforming realizations of Gaussian-related stochastic models while preserving their spatial variability. This method consists in building a stochastic process whose state space is the ensemble of the realizations of a spatial stochastic model. In particular, a stochastic process, built by combining independent Gaussian random functions, is proposed to perform the gradual deformation of realizations. Then, the gradual deformation algorithm is coupled with an optimization algorithm to calibrate realizations of stochastic models to nonlinear data. The method is applied to calibrate a continuous and a discrete synthetic permeability fields to well-test pressure data. The examples illustrate the efficiency of the proposed method. Furthermore, we present some extensions of this method (multidimensional gradual deformation, gradual deformation with respect to structural parameters, and local gradual deformation) that are useful in practice. Although the method described in this paper is operational only in the Gaussian framework (e.g., lognormal model, truncated Gaussian model, etc.), the idea of gradually deforming realizations through a stochastic process remains general and therefore promising even for calibrating non-Gaussian models},
author = {Hu, Lin Y.},
doi = {10.1023/A:1007506918588},
file = {:home/raphael/Documents/Paper-Mendeley/2000/Hu - 2000 - Gradual deformation and iterative calibration of Gaussian-related stochastic models.pdf:pdf},
issn = {08828121},
journal = {Mathematical Geology},
keywords = {Geostatistics,Heterogeneity,Inversion,Nonlinearity,Optimization,gradual deformation,history matching,inversion,nonlinearity,optimization},
mendeley-tags = {gradual deformation,history matching,inversion,nonlinearity,optimization},
number = {1},
pages = {87--108},
title = {{Gradual deformation and iterative calibration of Gaussian-related stochastic models}},
volume = {32},
year = {2000}
}
@article{Hu2001,
abstract = {This paper is about the application of the gradual deformation method$\backslash$nto the truncated pluri-Gaussian simulation. Global gradual deformation$\backslash$nwas used to optimize the facies distribution. The non-gradient based$\backslash$nmethod, golden-section search was indicated being used in optimizing$\backslash$nnon-differentible parameters. Some parameters are fixed in this project,$\backslash$nsuch as lithofacies proportions, variogram ranges etc. Threshold$\backslash$nmap composits lines parallel to the axes. A case study was presented$\backslash$nin this paper matching the well testing pressure and pressure derivative$\backslash$ndata from a well in the central layer. Objective function contains$\backslash$nonly the data mismatch. After around 10 iterations each, two realizations$\backslash$nwere generated with 6 percent and 0.1 percent of the starting objective$\backslash$nfunction values respectively.},
annote = {Apply Gradual Deformation for Sequential Simulation and not necessarly gaussian one.},
author = {Hu, Lin Y. and {Le Ravalec-Dupin}, Micka\"{e}le and Blanc, Georges},
doi = {10.1144/petgeo.7.S.S25},
file = {:home/raphael/Documents/Paper-Mendeley/2001/Hu, Le Ravalec-Dupin, Blanc - 2001 - Gradual deformation and iterative calibration of truncated \{Gaussian\} simulations.pdf:pdf},
issn = {1354-0793},
journal = {Petroleum Geoscience},
keywords = {geostatistics,gradual deformation,heterogeneity,history matching,nonlinearity,optimization,sequential simulation},
mendeley-tags = {gradual deformation,history matching,sequential simulation},
number = {4},
pages = {25--30},
title = {{Gradual deformation and iterative calibration of truncated \{Gaussian\} simulations}},
volume = {7},
year = {2001}
}
@article{Hu2002,
author = {Hu, Lin Y.},
doi = {10.1023/A:1021316707087},
file = {:home/raphael/Documents/Paper-Mendeley/2002/Hu - 2002 - Combination of dependent realizations within the gradual deformation method.pdf:pdf},
issn = {08828121},
journal = {Mathematical Geology},
keywords = {Conditional simulation,Cross-covariance,Optimization,Parameterization,Vector space,conditioning,dependence,gradual deformation},
mendeley-tags = {conditioning,dependence,gradual deformation},
number = {8},
pages = {953--963},
title = {{Combination of dependent realizations within the gradual deformation method}},
volume = {34},
year = {2002}
}
@article{Ferre2005,
author = {Ferr\'{e}, TPA and Binley, A and Geller, J and Hill, E and Illangasekare, T},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Ferr\'{e} et al. - 2005 - Hydrogeophysical methods at the laboratory scale.pdf:pdf},
journal = {Hydrogeophysics},
pages = {441--463},
title = {{Hydrogeophysical methods at the laboratory scale}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_15},
year = {2005}
}
@article{Kobr2005,
author = {Kobr, M and Mare\v{s}, S and Paillet, F},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Kobr, Mare\v{s}, Paillet - 2005 - Geophysical well logging.pdf:pdf},
journal = {Hydrogeophysics},
number = {1997},
pages = {291--331},
title = {{Geophysical well logging}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_10},
year = {2005}
}
@article{Caers2004,
abstract = {Caers J, Zhang T (2004) Multiple-point geostatistics: a quantitative vehicle for integrating geologic analogs into multiple reservoir models. In: Integration of outcrop and modern analog data in reservoir models. AAPG Mem 80:383–394},
author = {Caers, Jef},
file = {:home/raphael/Documents/Paper-Mendeley/2004/Caers - 2004 - Multiple-point geostatistics a quantitative vehicle for integrating geologic analogs into Stanford University , Stanford.pdf:pdf},
isbn = {0891813616},
issn = {02718529},
journal = {AAPG Mem 80:383–394},
pages = {383--394},
title = {{Multiple-point geostatistics : a quantitative vehicle for integrating geologic analogs into Stanford University , Stanford Center for Reservoir Forecasting}},
year = {2004}
}
@article{Ruggeri2013,
author = {Ruggeri, Paolo and Irving, James and Gloaguen, Erwan and Holliger, Klaus},
doi = {10.1093/gji/ggt067},
file = {:home/raphael/Documents/Paper-Mendeley/2013/Ruggeri et al. - 2013 - Regional-scale integration of multiresolution hydrological and geophysical data using a two-step Bayesian sequen.pdf:pdf},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {1 i n t,distribution of hydraulic conductivity,downhole methods,hydrogeophysics,knowledge of the detailed,permeability and porosity,probabilistic forecasting,ro d u c,t i o n,tomography},
month = apr,
number = {1},
pages = {289--303},
title = {{Regional-scale integration of multiresolution hydrological and geophysical data using a two-step Bayesian sequential simulation approach}},
url = {http://gji.oxfordjournals.org/cgi/doi/10.1093/gji/ggt067},
volume = {194},
year = {2013}
}
@article{Steeples2005,
author = {Steeples, DW},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Steeples - 2005 - Shallow seismic methods.pdf:pdf},
journal = {Hydrogeophysics},
number = {1991},
pages = {215--251},
title = {{Shallow seismic methods}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_8},
year = {2005}
}
@article{Hubbard2005,
author = {Hubbard, SS and Rubin, Y},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Hubbard, Rubin - 2005 - Introduction to hydrogeophysics.pdf:pdf},
journal = {Hydrogeophysics},
pages = {3--21},
title = {{Introduction to hydrogeophysics}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_1},
year = {2005}
}
@article{Gomes-Hernandez2005,
author = {Gomes-Hernandez, J. James},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Gomes-Hernandez - 2005 - Geostatistics.pdf:pdf},
journal = {Hydrogeophysics},
pages = {59--83},
title = {{Geostatistics}},
year = {2005}
}
@article{Hyndman2005,
author = {Hyndman, D and Tronicke, J},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Hyndman, Tronicke - 2005 - Hydrogeophysical case studies at the local scale The saturated zone.pdf:pdf},
journal = {Hydrogeophysics},
pages = {391--412},
title = {{Hydrogeophysical case studies at the local scale: The saturated zone}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_13},
year = {2005}
}
@article{Goldman2005,
author = {Goldman, MA and Gvirtzman, H and Meju, MA and Shtivelman, V},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Goldman et al. - 2005 - Hydrogeophysical case studies at the regional scale.pdf:pdf},
journal = {Hydrogeophysics},
pages = {361--389},
title = {{Hydrogeophysical case studies at the regional scale}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_12},
year = {2005}
}
@incollection{Jr2005,
author = {Jr, JJ Butler},
booktitle = {Hydrogeophysics},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Jr - 2005 - Hydrogeological methods for estimation of spatial variations in hydraulic conductivity.pdf:pdf},
pages = {23--58},
publisher = {Springer},
title = {{Hydrogeological methods for estimation of spatial variations in hydraulic conductivity}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_2},
year = {2005}
}
@article{Annan2005,
author = {Annan, AP},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Annan - 2005 - GPR methods for hydrogeological studies.pdf:pdf},
journal = {Hydrogeophysics},
pages = {185--213},
title = {{GPR methods for hydrogeological studies}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_7},
year = {2005}
}
@article{Paine2005,
author = {Paine, JG and Minty, BRS},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Paine, Minty - 2005 - Airborne hydrogeophysics.pdf:pdf},
journal = {Hydrogeophysics},
pages = {333--357},
title = {{Airborne hydrogeophysics}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_11},
year = {2005}
}
@article{Pride2005,
author = {Pride, SR},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Pride - 2005 - Relationships between seismic and hydrological properties.pdf:pdf},
journal = {Hydrogeophysics},
title = {{Relationships between seismic and hydrological properties}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_9},
year = {2005}
}
@article{Hu2004,
abstract = {Constraining stochastic models of reservoir properties such as porosity and permeability can be formulated as an optimization problem. While an optimization based on random search methods preserves the spatial variability of the stochastic model, it is prohibitively computer intensive. In contrast, gradient search methods may be very efficient but it does not preserve the spatial variability of the stochastic model. The gradual deformation method allows for modifying a reservoir model (i.e., realization of the stochastic model) from a small number of parameters while preserving its spatial variability. It can be considered as a first step towards the merger of random and gradient search methods. The gradual deformation method yields chains of reservoir models that can be investigated successively to identify an optimal reservoir model. The investigation of each chain is based on gradient computations, but the building of chains of reservoir models is random. In this paper, we propose an algorithm that further improves the efficiency of the gradual deformation method. Contrary to the previous gradual deformation method, we also use gradient information to build chains of reservoir models. The idea is to combine the initial reservoir model or the previously optimized reservoir model with a compound reservoir model. This compound model is a linear combination of a set of independent reservoir models. The combination coefficients are calculated so that the search direction from the initial model is as close as possible to the gradient search direction. This new gradual deformation scheme allows us for reducing the number of optimization parameters while selecting an optimal search direction. The numerical example compares the performance of the new gradual deformation scheme with that of the traditional one.},
author = {Hu, Lin Y. and {Le Ravalec-Dupin}, Micka\"{e}le},
doi = {10.1023/B:MATG.0000039542.73994.a2},
file = {:home/raphael/Documents/Paper-Mendeley/2004/Hu, Le Ravalec-Dupin - 2004 - An improved gradual deformation method for reconciling random and gradient searches in stochastic optimiza.pdf:pdf},
isbn = {08828121},
issn = {08828121},
journal = {Mathematical Geology},
keywords = {Geostatistical simulation,Inner product,Inverse problem,Projection,Sensitivity coefficients,Vector space,gradient-based search,gradual deformation,history matching},
mendeley-tags = {gradient-based search,gradual deformation,history matching},
number = {6},
pages = {703--719},
title = {{An improved gradual deformation method for reconciling random and gradient searches in stochastic optimizations}},
volume = {36},
year = {2004}
}
@article{Caers2006,
abstract = {Building of models in the Earth Sciences often requires the solution of an inverse problem: some unknown model parameters need to be calibrated with actual measurements. In most cases, the set of measurements cannot completely and uniquely determine the model parameters; hence multiple models can describe the same data set. Bayesian inverse theory provides a framework for solving this problem. Bayesian methods rely on the fact that the conditional probability of the model parameters given the data (the posterior) is proportional to the likelihood of observing the data and a prior belief expressed as a prior distribution of the model parameters. In case the prior distribution is not Gaussian and the relation between data and parameters (forward model) is strongly non-linear, one has to resort to iterative samplers, often Markov chain Monte Carlo methods, for generating samples that fit the data likelihood and reflect the prior model statistics. While theoretically sound, such methods can be slow to converge, and are often impractical when the forward model is CPU demanding. In this paper, we propose a new sampling method that allows to sample from a variety of priors and condition model parameters to a variety of data types. The method does not rely on the traditional Bayesian decomposition of posterior into likelihood and prior, instead it uses so-called pre-posterior distributions, i.e. the probability of the model parameters given some subset of the data. The use of pre-posterior allows to decompose the data into so-called, “easy data” (or linear data) and “difficult data” (or nonlinear data). The method relies on fast non-iterative sequential simulation to generate model realizations. The difficult data is matched by perturbing an initial realization using a perturbation mechanism termed “probability perturbation.” The probability perturbation method moves the initial guess closer tomatching the difficult data, while maintaining the priormodel statistics and the conditioning to the linear data. Several examples are used to illustrate the properties of this method.},
author = {Caers, Jef and Hoffman, Todd},
doi = {10.1007/s11004-005-9005-9},
file = {:home/raphael/Documents/Paper-Mendeley/2006/Caers, Hoffman - 2006 - The probability perturbation method A new look at Bayesian inverse modeling.pdf:pdf},
issn = {08828121},
journal = {Mathematical Geology},
keywords = {Bayes' theory,Geostatistics,History matching,Inverse modeling},
mendeley-tags = {Bayes' theory,Geostatistics,History matching,Inverse modeling},
number = {1},
pages = {81--100},
title = {{The probability perturbation method: A new look at Bayesian inverse modeling}},
volume = {38},
year = {2006}
}
@article{Caers2007,
abstract = {Inverse problems are ubiquitous in the Earth Sciences. Many such problems are ill-posed in the sense that multiple solutions can be found that match the data to be inverted. To impose restrictions on these solutions, a prior distribution of the model parameters is required. In a spatial context this prior model can be as simple as a Multi-Gaussian law with prior covariance matrix, or could come in the form of a complex training image describing the prior statistics of the model parameters. In this paper, two methods for generating inverse solutions constrained to such prior model are compared. The gradual deformation method treats the problem of finding inverse solution as an optimization problem. Using a perturbation mechanism, the gradual deformation method searches (optimizes) in the prior model space for those solutions that match the data to be inverted. The perturbation mechanism guarantees that the prior model statistics are honored. However, it is shown with a simple example that this perturbation method does not necessarily draw accurately samples from a given posterior distribution when the inverse problem is framed within a Bayesian context. On the other hand, the probability perturbation method approaches the inverse problem as a data integration problem. This method explicitly deals with the problem of combining prior probabilities with pre-posterior probabilities derived from the data. It is shown that the sampling properties of the probability perturbation method approach the accuracy of well-known Markov chain Monte Carlo samplers such as the rejection sampler. The paper uses simple examples to illustrate the clear differences between these two methods. © Springer Science+Business Media, LLC 2007.},
author = {Caers, Jef},
doi = {10.1007/s11004-006-9064-6},
file = {:home/raphael/Documents/Paper-Mendeley/2007/Caers - 2007 - Comparing the gradual deformation with the probability perturbation method for solving inverse problems.pdf:pdf},
isbn = {08828121},
issn = {08828121},
journal = {Mathematical Geology},
keywords = {Geostatistics,Inverse problems,Prior model},
number = {1},
pages = {27--52},
title = {{Comparing the gradual deformation with the probability perturbation method for solving inverse problems}},
volume = {39},
year = {2007}
}
@article{HendricksFranssen2009,
abstract = {Inverse modelling is a key step in groundwater-related hydrological studies. Several inversion techniques were developed during the last decades, but hardly any comparison between them was presented. We compare seven modern inverse methods for groundwater flow: the Regularised Pilot Points Method (both the estimation, RPPM-CE, and the Monte Carlo (MC) simulation variants, RPPM-CS), the MC variant of the Representer Method (RM), the Sequential Self-Calibration Method (SSC), the Moment Equations Method (MEM), the Zonation Method (ZM) and a non-iterative Semi-Analytical Method (SAM). These methods are applied to a two-dimensional synthetic example, depicting steady-state groundwater flow around a pumping well. Their relative performance is assessed in terms of their ability to characterise the log-transmissivity and hydraulic head fields and to predict the extent of the well catchment, both for a mildly and a strongly heterogeneous transmissivity field. The main conclusions drawn from the comparison are: (1) MC-based methods (RPPM-CS, SSC and RM) yield very similar results, regardless the degree of heterogeneity and despite they use different parameterisation schemes and objective functions; (2) statistical moments of the target quantities provided by MEM and RPPM-CE are similar to those of MC-based methods; (3) ZM and SAM are negatively affected by strong heterogeneity; and (4) in general, observed differences between the performances of all methods are not very large. MC-based inverse methods need considerably more CPU time than the other tested approaches. An advantage of MC-based methods is that they allow computing the posterior probability distribution of the target quantities, which can be directly fed to probabilistic risk-assessment procedures. © 2009 Elsevier Ltd. All rights reserved.},
author = {{Hendricks Franssen}, H. J. and Alcolea, a. and Riva, M. and Bakr, M. and van der Wiel, N. and Stauffer, F. and Guadagnini, a.},
doi = {10.1016/j.advwatres.2009.02.011},
file = {:home/raphael/Documents/Paper-Mendeley/2009/Hendricks Franssen et al. - 2009 - A comparison of seven methods for the inverse modelling of groundwater flow. Application to the chara.pdf:pdf},
isbn = {0309-1708},
issn = {03091708},
journal = {Advances in Water Resources},
keywords = {Aquifer characterisation,Comparison study,Conditional estimation,Inverse modelling,Stochastic simulations,Well catchments},
number = {6},
pages = {851--872},
publisher = {Elsevier Ltd},
title = {{A comparison of seven methods for the inverse modelling of groundwater flow. Application to the characterisation of well catchments}},
url = {http://dx.doi.org/10.1016/j.advwatres.2009.02.011},
volume = {32},
year = {2009}
}
@article{Franssen2009,
abstract = {Monte-Carlo (MC) type inverse modelling techniques like the sequential self-calibration (SSC) method, can be used as a benchmark to test other inverse modelling procedures. In comparison studies MC type inverse modelling methods outperformed other inverse parameter estimation methods, but the large amount of CPU time needed can become prohibitive to use such methods. Therefore, an interest exists to develop alternative methods that perform nearly as good and need much less CPU time. In this paper Ensemble Kalman Filtering (EnKF) is promoted for the off-line calibration of transient groundwater flow models with many nodes. An augmented state vector approach is used to calibrate parameters together with the updating of the states. EnKF and SSC are compared in two calibration experiments, for a mildly heterogeneous case ($\sigma$ln T
2 = 1.0) and a strongly heterogeneous case ($\sigma$ln T
2 = 2.7). For the mildly heterogeneous case, EnKF gives very similar results to SSC, also in terms of calibrated log-transmissivity fields. For the strongly heterogeneous case, EnKF gives still similar results as SSC, although the characterisation of the log T field improves less compared to SSC. On the other hand, EnKF needed in both cases around a factor of 80 less CPU time than SSC. In addition, the performance of EnKF and SSC was compared in two prediction experiments: (1) the prediction of groundwater flow in a different flow situation (without pumping and with a different time series of recharge rate), (2) the prediction of solute transport towards a pumping well. Both in the mildly and strongly heterogeneous cases the quality of the predictions with EnKF was as good as for SSC. Given the good performance of EnKF, the strength of EnKF to include multiple sources of uncertainty (e.g., related to external forcing) and the reduced CPU time needed compared to MC type inverse modelling, EnKF seems to be an interesting candidate for the stochastic calibration of large subsurface hydrological models. © 2008 Elsevier B.V. All rights reserved.},
author = {Franssen, H. J Hendricks and Kinzelbach, W.},
doi = {10.1016/j.jhydrol.2008.11.033},
file = {:home/raphael/Documents/Paper-Mendeley/2009/Franssen, Kinzelbach - 2009 - Ensemble Kalman filtering versus sequential self-calibration for inverse modelling of dynamic groundwater.pdf:pdf},
isbn = {0022-1694},
issn = {00221694},
journal = {Journal of Hydrology},
keywords = {Data assimilation,Ensemble Kalman Filtering,Groundwater hydrology,Inverse modelling,Sequential self-calibration,Transient groundwater flow},
number = {3-4},
pages = {261--274},
pmid = {263742400010},
publisher = {Elsevier B.V.},
title = {{Ensemble Kalman filtering versus sequential self-calibration for inverse modelling of dynamic groundwater flow systems}},
url = {http://dx.doi.org/10.1016/j.jhydrol.2008.11.033},
volume = {365},
year = {2009}
}
@article{Honarkhah2010,
abstract = {The advent of multiple-point geostatistics (MPS) gave rise to the integration of com- plex subsurface geological structures and features into the model by the concept of training images. Initial algorithms generate geologically realistic realizations by us- ing these training images to obtain conditional probabilities needed in a stochastic simulation framework. More recent pattern-based geostatistical algorithms attempt to improve the accuracy of the training image pattern reproduction. In these ap- proaches, the training image is used to construct a pattern database. Consequently, sequential simulation will be carried out by selecting a pattern from the database and pasting it onto the simulation grid. One of the shortcomings of the present algorithms is the lack of a unifying framework for classifying and modeling the patterns from the training image. In this thesis an entirely different approach will be taken towards geostatistical modeling. A novel, principled and unified technique for pattern analysis and generation that ensures computational efficiency and enables a straightforward incorporation of domain knowledge will be presented. In the developed methodology (called DisPAT), patterns scanned from the training image are represented as points in a Cartesian space using multi-dimensional scaling. The idea behind this mapping is to use distance functions as a tool for analyzing variability between all the patterns in a training image. These distance functions can be tailored to the application at hand. Next, by significantly reducing the dimensionality of the problem and using kernel space mapping, an improved pattern classification algorithm is obtained. Additionally, a multi-resolution approach is presented for modeling the patterns of the training image at various scales. The proposed distance-based pattern-modeling techniques are inspired by biology and the human visual system. Several examples are presented and a qualitative comparison is made with previous methods to demonstrate the capabilities of this simple, yet powerful system. We show how the proposed methodology is much less sensitive to the user-provided parameters, and at the same time has the potential to reduce computational time significantly. Another aspect of present MPS algorithms is their strong dependence on the algorithmic parameters that the practitioner specifies. This, not only entails tedious trial-and-error computations for tuning the parameters, but also triggers potential subjectivity in modeler’s decisions. In order to obtain a systematic pattern-based approach, new techniques on learning the optimal set of parameters are introduced. A series of examples is provided to verify the competency of these approaches in, not only facilitating the modeling process, but also ensuring a rigorous simulation framework. Furthermore, better data conditioning algorithms for both the hard data and the soft data are proposed. An improved pattern continuity and data-conditioning capability is observed in the generated realizations for both continuous and categorical variables. Some improvements in multi-scale data conditioning are also described. Overall, we demonstrate the higher conditioning capability of the proposed method in comparison with the tradi- tional algorithms. Finally, novel techniques on modeling non-stationary phenomena are introduced. The traditional approaches rely mainly on auxiliary variables to force the MPS algorithm into generating the desired spatial behavior; such as defining regions, constraining the facies proportions, or specifying the rotation/scaling of the features spatially. Rather in this thesis, the original MPS modeling paradigm, where a training image is the sole prerequisite for simulation, is re-established. The proposed framework conceptually embeds the spatial components of the patterns into geostatistical modeling. Various training images are used to demonstrate the capabilities of proposed approaches for modeling non-stationarity},
author = {Honarkhah, Mehrdad and Caers, Jef},
doi = {10.1007/s11004-010-9276-7},
file = {:home/raphael/Documents/Paper-Mendeley/2010/Honarkhah, Caers - 2010 - Stochastic simulation of patterns using distance-based pattern modeling.pdf:pdf},
isbn = {1874-8961},
issn = {18748961},
journal = {Mathematical Geosciences},
keywords = {Distance-based method,Geostatistics,Kernel,Mapping,Multiple point statistics,Pattern classification,Training image},
pages = {487--517},
title = {{Stochastic simulation of patterns using distance-based pattern modeling}},
volume = {42},
year = {2010}
}
@article{Cui2011,
abstract = {The aim of this research is to estimate the parameters of a large-scale$\backslash$nnumerical model of a geothermal reservoir using Markov chain Monte$\backslash$nCarlo (MCMC) sampling, within the framework of Bayesian inference.$\backslash$nAll feasible parameters that are consistent with the measured data$\backslash$nare summarized by the posterior distribution, and hence parameter$\backslash$nestimation and uncertainty quantification are both given by calculating$\backslash$nexpected values of statistics of interest over the posterior distribution.$\backslash$nIt appears to be computationally infeasible to use the standard Metropolis-Hastings$\backslash$nalgorithm (MH) to sample the high dimensional computationally expensive$\backslash$nposterior distribution. To improve the sampling efficiency, a new$\backslash$nadaptive delayed-acceptance MH algorithm (ADAMH) is implemented to$\backslash$nadaptively build a stochastic model of the error introduced by the$\backslash$nuse of a reduced-order model. This use of adaptivity differs from$\backslash$nexisting adaptive MCMC algorithms that tune proposal distributions$\backslash$nof the Metropolis-Hastings algorithm (MH), though ADAMH also implements$\backslash$nthat technique. For the 3-D geothermal reservoir model we present$\backslash$nhere, ADAMH shows a great improvement in the computational efficiency$\backslash$nof the MCMC sampling, and promising results for parameter estimation$\backslash$nand uncertainty quantification are obtained. This algorithm could$\backslash$noffer significant improvement in computational efficiency when implementing$\backslash$nsample-based inference in other large-scale inverse problems.},
author = {Cui, T. and Fox, C. and O'Sullivan, M. J.},
doi = {10.1029/2010WR010352},
file = {:home/raphael/Documents/Paper-Mendeley/2011/Cui, Fox, O'Sullivan - 2011 - Bayesian calibration of a large-scale geothermal reservoir model by a new adaptive delayed acceptance Metr.pdf:pdf},
issn = {00431397},
journal = {Water Resources Research},
keywords = {http://dx.doi.org/10.1029/2010WR010352, doi:10.102},
number = {September},
title = {{Bayesian calibration of a large-scale geothermal reservoir model by a new adaptive delayed acceptance Metropolis Hastings algorithm}},
volume = {47},
year = {2011}
}
@article{Wang2000,
abstract = {This study proposes and develops a streamline approach for inferring field-scale effective permeability distributions based on dynamic production data including producer water-cut curve, well pressures, and rates. The approach simplifies the history-matching process significantly. The basic idea is to relate the fractional-flow curve at a producer to the water breakthrough of individual streamlines. By adjusting the effective permeability along streamlines, the breakthrough time of each streamline is found that reproduces the reference producer fractional-flow curve. Then the permeability modification along each streamline is mapped onto cells of the simulation grid. Modifying effective permeability at the streamline level greatly reduces the size of the inverse problem compared to modifications at the grid-block level. The approach outlined here is relatively direct and greatly reduces the computational work by eliminating the repeated inversion of a system of equations. It works well for reservoirs where heterogeneity determines flow patterns. Example cases illustrate computational efficiency, generality, and robustness of the proposed procedure. Advantages and limitations of this work, and the scope of future study, are also discussed.},
author = {Wang, Yuandong and Kovscek, Anthony R},
doi = {10.2118/59370-MS},
file = {:home/raphael/Documents/Paper-Mendeley/2000/Wang, Kovscek - 2000 - A Streamline Approach for History-Matching Production Data.pdf:pdf},
isbn = {9781555633486},
pages = {3--5},
title = {{A Streamline Approach for History-Matching Production Data}},
year = {2000}
}
@article{Caers2001,
abstract = {The traditional practice of geostatistics for reservoir characterization is limited by the variogram which, as a measure of geological continuity, can capture only two-point statistics. Important curvi-linear geological information, beyond the modelling capabilities of the variogram, can be taken from training images and later used in model construction. Training images can provide multiple-point statistics which describe the statistical relation between multiple spatial locations considered jointly. Stochastic reservoir simulation then consists of anchoring the borrowed gee-structures in the form of multiple-point statistics to the actual subsurface hard and soft data. (C) 2001 Elsevier Science B.V. All rights reserved.},
author = {Caers, Jef},
doi = {10.1016/S0920-4105(01)00088-2},
file = {:home/raphael/Documents/Paper-Mendeley/2001/Caers - 2001 - Geostatistical reservoir modelling using statistical pattern recognition.pdf:pdf},
isbn = {1650723806},
issn = {09204105},
journal = {Journal of Petroleum Science and Engineering},
keywords = {geostatistics,inverse and forward modelling,multiple-point statistics,neural networks},
pages = {177--188},
title = {{Geostatistical reservoir modelling using statistical pattern recognition}},
url = {<Go to ISI>://000169287000003},
volume = {29},
year = {2001}
}
@article{Jef2002,
author = {Caers, Jef},
doi = {10.2523/77429-MS},
file = {:home/raphael/Documents/Paper-Mendeley/2002/Caers - 2002 - Methods for history matching under geological constraints.pdf:pdf},
journal = {8th European Conference on the Mathematics of Oil Recovery},
keywords = {gradient-based optimization,gradual deformation,history matching,objectif function,probability perturbation,sequential simulation,streamline method},
mendeley-tags = {gradient-based optimization,gradual deformation,history matching,objectif function,probability perturbation,sequential simulation,streamline method},
number = {September},
pages = {3--6},
title = {{Methods for history matching under geological constraints}},
url = {http://www.spe.org/elibrary/servlet/spepreview?id=00077429},
year = {2002}
}
@article{Caers2003a,
abstract = {The application of state-of-the-art history matching methods to large heterogeneous reservoirs is hampered by two main problems: (1) reservoir models should be constrained jointly to dynamic data and a large variety of geological continuity information, and (2) CPU demand should not be prohibitive for large models. This paper proposes a method contributing to alleviating these two main concerns. By combining three existing ideas, namely gradual deformation, multiple-point geostatistics and a fast streamline-based history matching method, an algorithm is proposed that could honor a large variety of geological scenarios and is obtained with a limited amount of flow simulations. The method expands on the traditional gradual deformation methodology in three ways: (1) multiple-point geostatistics is used to generate models that are geologically more realistic than traditional variogram-based models, (2) a streamline simulator is used to define "zones-of-influence" of producers in order to locally deform an initial model toward jointly matching a large amount of wells and most importantly (3) the number of flow calculations is limited by defining a proxy to the streamline simulator in terms of streamline-based harmonic averages. Using synthetic examples of increasing complexity, the method's efficiency and generality is assessed. ?? 2003 Elsevier Science B.V. All rights reserved.},
author = {Caers, Jef},
doi = {10.1016/S0920-4105(03)00040-8},
file = {:home/raphael/Documents/Paper-Mendeley/2003/Caers - 2003 - Efficient gradual deformation using a streamline-based proxy method.pdf:pdf},
issn = {09204105},
journal = {Journal of Petroleum Science and Engineering},
keywords = {Gradual deformation,History matching,Multiple-point geostatistics,Streamlines},
pages = {57--83},
title = {{Efficient gradual deformation using a streamline-based proxy method}},
volume = {39},
year = {2003}
}
@incollection{Lesmes2005,
author = {Lesmes, DP and Friedman, SP},
booktitle = {Hydrogeophysics},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Lesmes, Friedman - 2005 - Relationships between the electrical and hydrogeological properties of rocks and soils.pdf:pdf},
pages = {87--128},
publisher = {Springer},
title = {{Relationships between the electrical and hydrogeological properties of rocks and soils}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_4},
year = {2005}
}
@article{Daniels2005,
author = {Daniels, JJ and Allred, B and Binley, A},
file = {:home/raphael/Documents/Paper-Mendeley/2005/Daniels, Allred, Binley - 2005 - Hydrogeophysical case studies in the vadose zone.pdf:pdf},
journal = {Hydrogeophysics},
pages = {413--440},
title = {{Hydrogeophysical case studies in the vadose zone}},
url = {http://link.springer.com/chapter/10.1007/1-4020-3102-5\_14},
year = {2005}
}
@article{Caers2003,
abstract = {History matching forms an integral part of the reservoir modeling workflow process. Despite the existence of many history-matching tools, the integration of production data with seismic and geological continuity data remains a challenge. Geostatistical tools now are routinely employed for integrating large-scale seismic and fine-scale well/core data. A general framework for integrating production data with diverse types of geological/structural data is largely lacking. In this paper, we develop a new method for history matching that can account for production data constraint by prior geological data, such as the presence of channels, fractures, or shale lenses. With multiple-point (MP) geostatistics, prior information about geological patterns is carried by training images from which geological structures are borrowed, then anchored to the subsurface data. A simple Markov chain is proposed to iteratively modify the MP geostatistical realizations until history match. The method is simple and general in the sense that the procedure can be applied to a wide variety of geological environments without requiring a modification of the algorithm. The method does not make assumptions on the flow model. Synthetic cases are used to assess the flexibility of the proposed method.},
author = {Caers, Jef},
doi = {10.2118/74716-PA},
file = {:home/raphael/Documents/Paper-Mendeley/2003/Caers - 2003 - History Matching Under Training-Image-Based Geological Model Constraints.pdf:pdf},
isbn = {9781555632106},
issn = {1086055X},
journal = {SPE Journal},
keywords = {history matching,probability perturbation,snesim,training-image},
mendeley-tags = {history matching,probability perturbation,snesim,training-image},
pages = {1--39},
title = {{History Matching Under Training-Image-Based Geological Model Constraints}},
volume = {8},
year = {2003}
}
@article{Arpat2007,
abstract = {An entirely new approach to stochastic simulation is proposed through the direct simulation of patterns. Unlike pixel-based (single grid cells) or object-based stochastic simulation, pattern-based simulation simulates by pasting patterns directly onto the simulation grid. A pattern is a multi-pixel configuration identifying a meaningful entity (a puzzle piece) of the underlying spatial continuity. The methodology relies on the use of a training image from which the pattern set (database) is extracted. The use of training images is not new. The concept of a training image is extensively used in simulating Markov random fields or for sequentially simulating structures using multiple-point statistics. Both these approaches rely on extracting statistics from the training image, then reproducing these statistics in multiple stochastic realizations, at the same time conditioning to any available data. The proposed approach does not rely, explicitly, on either a statistical or probabilistic methodology. Instead, a sequential simulation method is proposed that borrows heavily from the pattern recognition literature and simulates by pasting at each visited location along a random path a pattern that is compatible with the available local data and any previously simulated patterns. This paper discusses the various implementation details to accomplish this idea. Several 2D illustrative as well as realistic and complex 3D examples are presented to showcase the versatility of the proposed algorithm.},
author = {Arpat, G. Burc and Caers, Jef},
doi = {10.1007/s11004-006-9075-3},
file = {:home/raphael/Documents/Paper-Mendeley/2007/Arpat, Caers - 2007 - Conditional simulation with patterns.pdf:pdf},
isbn = {1100400690753},
issn = {08828121},
journal = {Mathematical Geology},
keywords = {Geostatistics,Multiple-pointstatistics,Pattern similarity},
number = {2},
pages = {177--203},
title = {{Conditional simulation with patterns}},
volume = {39},
year = {2007}
}
@article{Ruggeri2014,
author = {Ruggeri, Paolo and Gloaguen, Erwan and Lefebvre, Ren\'{e} and Irving, James and Holliger, Klaus},
doi = {10.1016/j.jhydrol.2014.04.031},
file = {:home/raphael/Documents/Paper-Mendeley/2014/Ruggeri et al. - 2014 - Integration of hydrological and geophysical data beyond the local scale Application of Bayesian sequential simu.pdf:pdf},
issn = {00221694},
journal = {Journal of Hydrology},
month = jun,
pages = {271--280},
publisher = {Elsevier B.V.},
title = {{Integration of hydrological and geophysical data beyond the local scale: Application of Bayesian sequential simulation to field data from the Saint-Lambert-de-Lauzon site, Qu\'{e}bec, Canada}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022169414003059},
volume = {514},
year = {2014}
}
